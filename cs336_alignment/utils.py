import re
import json
from pathlib import Path

from typing import List, Callable
from vllm import LLM, SamplingParams
from transformers import PreTrainedModel, PreTrainedTokenizerBase


def extract_gsm8k_answer(answer: str) -> str:
    return re.search(r"####\s*([\-0-9]+)", answer).group(1).strip()


def format_gsm8k_answer(answer: str) -> str:
    m = re.search(r"####\s*([^\n]+)\s*$", answer)

    ans = m.group(1).strip()
    prefix = answer[: m.start()].rstrip()
    return f"{prefix} </think> <answer>{ans}</answer>"


def load_gsm8k_data(prompt_path, data_path):
    with open(prompt_path, "r") as file:
        prompt_template = file.read()

    prompts, responses, answers = [], [], []
    with open(data_path, "r") as file:
        for line in file:
            item = json.loads(line)
            prompts.append(prompt_template.format(question=item["question"]))
            responses.append(format_gsm8k_answer(item["answer"]))
            answers.append(extract_gsm8k_answer(item["answer"]))

    return prompts, responses, answers


def evaluate_vllm(
    vllm_model: LLM,
    reward_fn: Callable[[str, str], dict[str, float]],
    prompts: List[str],
    answers: List[str],
    eval_sampling_params: SamplingParams,
) -> List[dict]:
    """
    Evaluate a language model on a list of prompts,
    compute evaluation metrics, and serialize results to disk.
    """

    outputs = vllm_model.generate(prompts, eval_sampling_params)

    results = []
    for prompt, answer, output in zip(prompts, answers, outputs):
        for out in output.outputs:
            generated_text = out.text.strip()
            rewards = reward_fn(generated_text, answer)
            results.append(
                {
                    "prompt": prompt,
                    "generated_text": generated_text,
                    "answer": answer,
                    "rewards": rewards,
                }
            )

    return results


def log_generations(
    results: List[dict],
    save_path: Path | None = None,
):
    """
    Log generations, including:
        1. The input prompt.
        2. The response generated by the SFT/RL model.
        3. The ground-truth answer.
        4. The reward information, including format, answer, and total reward.
        5. The average response length, average response length for correct responses, and average response length
            for incorrect responses.
    """
    if save_path is not None:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        file = open(save_path, "w", encoding="utf-8")
    else:
        file = None

    total_count = len(results)
    correct_both = format_only = neither = 0

    lengths = []
    correct_lengths = []
    incorrect_lengths = []

    for result in results:
        gen_text = result["generated_text"]
        rewards = result["rewards"]
        format_reward = rewards.get("format_reward", 0)
        answer_reward = rewards.get("answer_reward", 0)

        text_length = len(gen_text.split())
        if format_reward == 1 and answer_reward == 1:
            correct_both += 1
            correct_lengths.append(text_length)
        elif format_reward == 1 and answer_reward == 0:
            format_only += 1
            incorrect_lengths.append(text_length)
        elif format_reward == 0 and answer_reward == 0:
            neither += 1
            incorrect_lengths.append(text_length)

        lengths.append(text_length)

        if file is not None:
            json.dump(result, file, ensure_ascii=False)
            file.write("\n")

    avg_len = sum(lengths) / len(lengths) if lengths else 0
    avg_len_correct = (
        sum(correct_lengths) / len(correct_lengths) if correct_lengths else 0
    )
    avg_len_incorrect = (
        sum(incorrect_lengths) / len(incorrect_lengths) if incorrect_lengths else 0
    )

    if file is not None:
        file.close()

    return {
        "total_count": total_count,
        "correct_both": correct_both,
        "format_only": format_only,
        "neither": neither,
        "avg_len": avg_len,
        "avg_len_correct": avg_len_correct,
        "avg_len_incorrect": avg_len_incorrect,
    }


def save_model(
    dir_name: str,
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizerBase | None = None,
):
    """
    Save a model and optionally its tokenizer.

    Args:
        dir_name: Name for the saved model directory
        model: The model to save
        tokenizer: The tokenizer to save (optional)

    Returns:
        Path: The path where the model was saved
    """
    current_dir = Path(__file__).parent
    outputs_dir = current_dir.parent / "models"
    outputs_dir.mkdir(exist_ok=True)

    model_save_path = outputs_dir / dir_name

    model.save_pretrained(model_save_path)

    if tokenizer is not None:
        tokenizer.save_pretrained(model_save_path)

    return model_save_path
